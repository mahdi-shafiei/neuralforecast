



# <kbd>module</kbd> `neuralforecast.auto`






---



## <kbd>class</kbd> `AutoRNN`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c12df30>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```

Auto RNN 

**Parameters:**<br/> 


---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoLSTM`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c12e470>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoGRU`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c12ea40>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoTCN`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c12f010>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoDeepAR`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=DistributionLoss(),
    valid_loss=MQLoss(),
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c12fb80>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoDilatedRNN`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c12fc10>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoBiTCN`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c0c8370>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoxLSTM`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c0c8970>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoMLP`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c0c9090>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoNBEATS`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c0c9690>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoNBEATSx`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c0cbc40>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoNHITS`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c0cb400>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoDLinear`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c0cae30>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoNLinear`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c0ca8f0>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoTiDE`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f757c0c9f00>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoDeepNPTS`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0d8d7b0>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoKAN`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0d8f850>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoTFT`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0d8fbb0>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoVanillaTransformer`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0d8f430>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoInformer`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0d8ee30>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoAutoformer`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0d8e5f0>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoFEDformer`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0d8de70>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoPatchTST`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0dc00a0>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoiTransformer`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    n_series,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0dc0850>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series)
```






---



## <kbd>class</kbd> `AutoTimeXer`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    n_series,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0dc0dc0>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series)
```






---



## <kbd>class</kbd> `AutoTimesNet`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0dc1330>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoStemGNN`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    n_series,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0dc1870>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series)
```






---



## <kbd>class</kbd> `AutoHINT`






### <kbd>method</kbd> `__init__`

```python
__init__(
    cls_model,
    h,
    loss,
    valid_loss,
    S,
    config,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0dc1e40>,
    num_samples=10,
    cpus=4,
    gpus=0,
    refit_with_val=False,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series=None)
```






---



## <kbd>class</kbd> `AutoTSMixer`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    n_series,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0dc1ed0>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series)
```






---



## <kbd>class</kbd> `AutoTSMixerx`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    n_series,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0dc24d0>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series)
```






---



## <kbd>class</kbd> `AutoMLPMultivariate`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    n_series,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0dc2b90>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series)
```






---



## <kbd>class</kbd> `AutoSOFTS`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    n_series,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0dc3100>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series)
```






---



## <kbd>class</kbd> `AutoTimeMixer`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    n_series,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0dc3d90>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series)
```






---



## <kbd>class</kbd> `AutoRMoK`






### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    n_series,
    loss=MAE(),
    valid_loss=None,
    config=None,
    search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f75a0dc3790>,
    num_samples=10,
    refit_with_val=False,
    cpus=4,
    gpus=0,
    verbose=False,
    alias=None,
    backend='ray',
    callbacks=None
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>classmethod</kbd> `get_default_config`

```python
get_default_config(h, backend, n_series)
```






